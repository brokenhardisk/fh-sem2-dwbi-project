{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to large volume of input data, we cannot upload it to Github.\n",
    "As a workaround, we will load the data into Postgres DB locally.\n",
    "As a first step, we will load the required dependencies and create the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('../credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "    psql_config = {\n",
    "    'dbname': data['db_name'],\n",
    "    'user': data['db_user'],\n",
    "    'password': data['db_pwd'],\n",
    "    'host': data['db_host'],\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "\n",
    "def get_psql_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**psql_config)\n",
    "    except Exception as e:\n",
    "        print(\"Error connecting to the database:\", e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define and execute the functions to create the staging table and truncate it incase it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_citibike_data_table():\n",
    "    try:\n",
    "        statement = \"\"\"truncate table m024._bike_data;\"\"\"\n",
    "        with get_psql_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(statement)\n",
    "            conn.commit()  \n",
    "            print(f\"citibike_data table truncated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in truncating citibike table - {e}\")\n",
    "\n",
    "def create_citibike_data_table():\n",
    "    try:\n",
    "        statement = \"\"\"create table if not exists m024.citi_bike_data\n",
    "(\n",
    "    tripduration            integer,\n",
    "    starttime               timestamp,\n",
    "    stoptime                timestamp,\n",
    "    start_station_id        integer,\n",
    "    start_station_name      varchar(255),\n",
    "    start_station_latitude  double precision,\n",
    "    start_station_longitude double precision,\n",
    "    end_station_id          integer,\n",
    "    end_station_name        varchar(255),\n",
    "    end_station_latitude    double precision,\n",
    "    end_station_longitude   double precision,\n",
    "    bikeid                  integer,\n",
    "    usertype                varchar(50),\n",
    "    birth_year              integer,\n",
    "    gender                  integer,\n",
    "    processed               boolean\n",
    ");\n",
    "        \"\"\"\n",
    "        with get_psql_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(statement)\n",
    "            conn.commit()  \n",
    "            print(f\"citibike_data table created successfully/already exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating citibike table - {e}\")\n",
    "\n",
    "def insert_data_to_citibike_table(df):\n",
    "    try:\n",
    "        with get_psql_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Prepare the insert statement\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO m024.citi_bike_data (\n",
    "                    tripduration, starttime, stoptime, start_station_id, start_station_name, \n",
    "                    start_station_latitude, start_station_longitude, end_station_id, \n",
    "                    end_station_name, end_station_latitude, end_station_longitude, bikeid, \n",
    "                    usertype, birth_year, gender\n",
    "                ) \n",
    "                VALUES %s\n",
    "                ON CONFLICT DO NOTHING;\n",
    "                \"\"\"\n",
    "                # Convert DataFrame rows into tuples\n",
    "                rows = [\n",
    "                    (\n",
    "                        row['tripduration'], row['starttime'], row['stoptime'], row['start station id'], \n",
    "                        row['start station name'], row['start station latitude'], row['start station longitude'], row['end station id'],\n",
    "                        row['end station name'], row['end station latitude'], row['end station longitude'],\n",
    "                        row['bikeid'], row['usertype'], row['birth year'], row['gender']\n",
    "                    )\n",
    "                    for _, row in df.iterrows()\n",
    "                ]\n",
    "\n",
    "                # Use psycopg2.extras.execute_values for efficient bulk insert\n",
    "                from psycopg2.extras import execute_values\n",
    "                execute_values(cur, insert_query, rows)\n",
    "            \n",
    "            conn.commit()\n",
    "            #print(f\"Data inserted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in inserting data: {e}\") \n",
    "\n",
    "def load_processed_files(log_file=\"processed_files.csv\"):\n",
    "    \"\"\"Load the processed files list from the log file.\"\"\"\n",
    "    if os.path.exists(log_file):\n",
    "        processed_files_df = pd.read_csv(log_file)\n",
    "        return set(processed_files_df['file_name'].tolist())\n",
    "    else:\n",
    "        # If no log file exists, return an empty set\n",
    "        return set()\n",
    "\n",
    "def save_processed_file(file_name, log_file=\"processed_files.csv\"):\n",
    "    \"\"\"Save the processed file to the log.\"\"\"\n",
    "    processed_files_df = pd.DataFrame({'file_name': [file_name]})\n",
    "    if os.path.exists(log_file):\n",
    "        processed_files_df.to_csv(log_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        processed_files_df.to_csv(log_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citibike_data table created successfully/already exists\n",
      "Error in truncating citibike table - relation \"m024._bike_data\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(f'postgresql://{psql_config[\"user\"]}:{psql_config[\"password\"]}@{psql_config[\"host\"]}/{psql_config[\"dbname\"]}')\n",
    "\n",
    "create_citibike_data_table()\n",
    "truncate_citibike_data_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will start processing the csv files one by one and maintain a log to track the already processed files to avoid re-processing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=\"../2018-citibike-tripdata\"\n",
    "log_file=\"processed_files.csv\"\n",
    "\n",
    "csv_files = sorted([file for file in os.listdir(folder_path) if file.endswith(\"tripdata.csv\")])\n",
    "processed_files = load_processed_files(log_file)\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files in alphabetical order\"):\n",
    "    try:\n",
    "        # Skip already processed files\n",
    "        if file in processed_files:\n",
    "            print(f\"Skipping already processed file: {file}\")\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        df = pd.read_csv(file_path,on_bad_lines='skip')\n",
    "\n",
    "        required_columns = [\n",
    "            'tripduration', 'starttime', 'stoptime', 'start station id', \n",
    "            'start station name', 'start station latitude', \n",
    "            'start station longitude', 'end station id', 'end station name', \n",
    "            'end station latitude', 'end station longitude'\n",
    "        ]\n",
    "        if not set(required_columns).issubset(df.columns):\n",
    "            raise ValueError(f\"Missing columns in {file_path}. Required Columns: {required_columns} and columns in file {df.columns}\")\n",
    "        insert_data_to_citibike_table(df)\n",
    "        # After processing, log the file as processed\n",
    "        save_processed_file(file, log_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
